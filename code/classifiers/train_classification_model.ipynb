{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the data directory    let say you can set wheat or rice train data \n",
    "# Access all files into a list\n",
    "data_dir = 'wheat_data/train'   \n",
    "\n",
    "# getting folder paths\n",
    "folder_paths = []\n",
    "folder_names = []\n",
    "for fold_name in os.listdir(data_dir):\n",
    "    fold_path = os.path.join(data_dir,fold_name)\n",
    "    if os.path.isdir(fold_path):\n",
    "        folder_paths.append(fold_path)\n",
    "        folder_names.append(fold_name)\n",
    "\n",
    "## defining a dataframe to make a list of image id, its path and associated label \n",
    "columns =  ['img_id','img_path','label']\n",
    "train_df = pd.DataFrame(columns=columns)\n",
    "i = 0\n",
    "fold_ind = 0\n",
    "for ind,folder in enumerate(folder_paths):\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        if os.path.isfile(img_path):\n",
    "            train_df.loc[i]=[img_name,img_path,folder_names[ind]]\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover how many classes are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_df.label.nunique(),'label')\n",
    "label_counts  = train_df.label.value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the images available are in different file extensions then make same type and desired resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_train_dir= 'train_2'\n",
    "for imglist in tqdm(train_df.values):\n",
    "    img_name = imglist[0]\n",
    "    img_path =imglist[1]\n",
    "    label=imglist[2]\n",
    "    label_dir = os.path.join(dest_train_dir,label)\n",
    "    img_name  = img_name.partition('.')[0]\n",
    "    dest = os.path.join(label_dir,img_name + '.png')\n",
    "    img = load_img(img_path)\n",
    "    img  = img.resize((512,512))\n",
    "    if not os.path.exists(label_dir):\n",
    "        os.makedirs(label_dir)\n",
    "    img.save(dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have 'train_2' as main training data set it as data_dir again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'wheat_data/train_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the training, testing and validation sets\n",
    "set_resize = 128 \n",
    "\n",
    "data_transforms ={\n",
    "    \"train_transforms\": transforms.Compose([transforms.Resize(set_resize),\n",
    "                                           transforms.RandomRotation(30),\n",
    "                                           #transforms.RandomResizedCrop(128), \n",
    "                                           transforms.RandomHorizontalFlip(), \n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                [0.229, 0.224, 0.225])]),\n",
    "   \"valid_transforms\": transforms.Compose([transforms.Resize(set_resize),\n",
    "                                           #transforms.CenterCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                [0.229, 0.224, 0.225])]), \n",
    "    \"test_transforms\": transforms.Compose([transforms.Resize(set_resize),\n",
    "                                           #transforms.CenterCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                [0.229, 0.224, 0.225])])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation and test\n",
    "train_data = 0.1\n",
    "valid_data = 0.05\n",
    "test_data = 0.05\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(data_dir, transform=data_transforms[\"train_transforms\"])#loading dataset\n",
    "valid_data = datasets.ImageFolder(data_dir, transform=data_transforms[\"valid_transforms\"])\n",
    "test_data = datasets.ImageFolder(data_dir, transform=data_transforms[\"test_transforms\"])\n",
    "\n",
    "# Obtain training indices that will be used for validation and test\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_count = int(0.8*num_train)\n",
    "valid_count = int(0.1*num_train)\n",
    "test_count = num_train - train_count - valid_count\n",
    "train_idx = indices[:train_count]\n",
    "valid_idx = indices[train_count:train_count+valid_count]\n",
    "test_idx = indices[train_count+valid_count:]\n",
    "\n",
    "print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "print(\"Training\", train_count, np.sum(len(train_idx)/num_train))\n",
    "print(\"Validation\", valid_count, np.sum(len(valid_idx)/num_train))\n",
    "print(\"Test\", test_count, np.sum(len(test_idx)/num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom sampler for the dataset loader avoiding recreating the dataset (just creating a new loader for each different sampling)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders using the image datasets. Dataloader is used to load our data in batches\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size = 64, sampler=train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size = 32, sampler = valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size = 32, sampler = test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['healthy_wheat', 'leaf_rust', 'stem_rust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model architecture\n",
    "# Load the pretrained model from pytorch's library and stored it in model_transfer\n",
    "model_transfer = models.googlenet(pretrained=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "# Check if GPU is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print the model to see all the layers\n",
    "print(model_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets read the fully connected layer\n",
    "print(model_transfer.fc.in_features)\n",
    "print(model_transfer.fc.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define n_inputs takes the same number of inputs from pre-trained model\n",
    "n_inputs = model_transfer.fc.in_features #refer to the fully connected layer only\n",
    "\n",
    "# Add last linear layer (n_inputs -> 4 classes). In this case the ouput is 4 classes\n",
    "# New layer automatically has requires_grad = True\n",
    "last_layer = nn.Linear(n_inputs, len(classes))\n",
    "\n",
    "model_transfer.fc = last_layer\n",
    "\n",
    "# If GPU is available, move the model to GPU\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "  \n",
    "# Check to see the last layer produces the expected number of outputs\n",
    "print(model_transfer.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function and optimizer\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "optimizer_transfer = optim.SGD(model_transfer.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path='trained_models/googlenet.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    '''returns trained model'''\n",
    "    # Initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.inf\n",
    "  \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # In the training loop, I track down the loss\n",
    "        # Initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "    \n",
    "        # Model training\n",
    "        model.train()\n",
    "        for batch_idx, (data,target) in enumerate(trainloader):\n",
    "            # 1st step: Move to GPU\n",
    "            if use_cuda:\n",
    "                data,target = data.cuda(), target.cuda()\n",
    "      \n",
    "            # Then, clear (zero out) the gradient of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # Perform the Cross Entropy Loss. Calculate the batch loss.\n",
    "            loss = criterion(output, target)\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Perform optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # Record the average training loss\n",
    "            train_loss = train_loss + ((1/ (batch_idx + 1 ))*(loss.data-train_loss))\n",
    "      \n",
    "        # Model validation\n",
    "        model.eval()\n",
    "        for batch_idx, (data,target) in enumerate(validloader):\n",
    "            # Move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # Update the average validation loss\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # Calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # Update the average validation loss\n",
    "            valid_loss = valid_loss + ((1/ (batch_idx +1)) * (loss.data - valid_loss))\n",
    "      \n",
    "        # print training/validation stats\n",
    "        print('Epoch: {} \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(\n",
    "            epoch,\n",
    "            train_loss,\n",
    "            valid_loss))\n",
    "    \n",
    "        # Save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.5f} --> {:.5f}). Saving model ...'.format(\n",
    "                  valid_loss_min,\n",
    "                  valid_loss))\n",
    "            torch.save(model.state_dict(),save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "  \n",
    "    # Return trained model\n",
    "    return model\n",
    "\n",
    "# Define loaders transfer\n",
    "loaders_transfer = {'train': trainloader,\n",
    "                    'valid': validloader,\n",
    "                    'test': testloader}\n",
    "\n",
    "# Train the model\n",
    "model_transfer = train(50, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model_transfer.eval() #set model into evaluation/testing mode. It turns of drop off layer\n",
    "    #Iterating over test data\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to \n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
